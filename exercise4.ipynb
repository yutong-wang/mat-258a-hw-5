{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "minimizeAL (generic function with 2 methods)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##############\n",
    "# Minimizers #\n",
    "##############\n",
    "\n",
    "# Note: All minimizer functions returns an array L such that\n",
    "# L[1] = x^*, or the optimal point\n",
    "# L[2] = value of the objective function at the optimal point\n",
    "# L[3] = number of iterations used or DEFAULT_MAX_ITER\n",
    "\n",
    "const DEFAULT_MAX_ITER = 100\n",
    "const DEFAULT_TOLERANCE = 0.01\n",
    "\n",
    "# Minimization algorithm using Newton's method\n",
    "# f  the objective function\n",
    "# g  the gradient of f\n",
    "# h  the Hessian of f\n",
    "# x  an initial testing point\n",
    "# tolerance a small number\n",
    "function minimizenewt(f,g, H, x::Vector, tolerance=DEFAULT_TOLERANCE, max_iter = DEFAULT_MAX_ITER)\n",
    "    alpha =0.3\n",
    "    beta = 0.7\n",
    "    iter = 0\n",
    "    \n",
    "    # Default mode is RELATIVE tolerance, comment out the line below for ABSOLUTE tolerance\n",
    "    tolerance = tolerance*norm(g(x))\n",
    "\n",
    "    while norm(g(x)) > tolerance && iter < max_iter\n",
    "        dx = -H(x)\\g(x)\n",
    "        t = 1\n",
    "        while f(x+t*dx) > f(x) + alpha*t*sum(g(x).*dx)\n",
    "            t = beta*t\n",
    "        end\n",
    "        x = x + t*dx\n",
    "        iter = iter + 1\n",
    "    end\n",
    "    if iter < max_iter\n",
    "        success =1\n",
    "    else\n",
    "        success =0\n",
    "    end\n",
    "    if iter > max_iter\n",
    "        error(\"Exceeding max iterations\")\n",
    "    end\n",
    "    return x\n",
    "end\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Minimization algorithm using Newton's method, with fix for nonpositive definite matrix\n",
    "# f  the objective function\n",
    "# g  the gradient of f\n",
    "# h  the Hessian of f\n",
    "# x  an initial testing point\n",
    "# tolerance a small number\n",
    "function minimizenewtnpd(f,g, H, x::Vector, tolerance=DEFAULT_TOLERANCE, max_iter = DEFAULT_MAX_ITER)\n",
    "    alpha =0.3\n",
    "    beta = 0.7\n",
    "    iter = 0\n",
    "    function fixnonposdefmat(A, epsilon=0.1)\n",
    "        M = eigfact(A)\n",
    "        V = M[:vectors]\n",
    "        L = M[:values]\n",
    "        X = [max(abs(vi), epsilon)::Float64 for vi in L]\n",
    "        L = diagm(X)\n",
    "        return (V'*L*V)\n",
    "    end\n",
    "    \n",
    "    # Default mode is RELATIVE tolerance, comment out the line below for ABSOLUTE tolerance\n",
    "    tolerance = tolerance*sqrt(sum(g(x).^2))\n",
    "\n",
    "    while sqrt(sum(g(x).^2)) > tolerance && iter < max_iter\n",
    "        if isposdef(H(x))\n",
    "            HH = H(x)\n",
    "        else\n",
    "            HH = fixnonposdefmat(H(x))\n",
    "        end\n",
    "        dx = -HH\\g(x)\n",
    "        t = 1\n",
    "        while f(x+t*dx) > f(x) + alpha*t*sum(g(x).*dx)\n",
    "            t = beta*t\n",
    "        end\n",
    "        x = x + t*dx\n",
    "        iter = iter + 1\n",
    "    end\n",
    "    if iter > max_iter\n",
    "        error(\"Exceeding max iterations\")\n",
    "    end\n",
    "    println(f(x))\n",
    "    println(\"iterations: $iter\")\n",
    "    return x\n",
    "end\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Minimization algorithm using Newton's method, with fix for nonpositive definite matrix, version 2\n",
    "# f  the objective function\n",
    "# g  the gradient of f\n",
    "# h  the Hessian of f\n",
    "# x  an initial testing point\n",
    "# tolerance a small number\n",
    "function minimizenewtnpd2(f,g, H, x::Vector, tolerance=DEFAULT_TOLERANCE, max_iter = DEFAULT_MAX_ITER)\n",
    "    alpha =0.3\n",
    "    beta = 0.7\n",
    "    iter = 0\n",
    "    function fixnonposdefmat(A, epsilon=0.1)\n",
    "        M = eigfact(A)\n",
    "        V = M[:vectors]\n",
    "        L = M[:values]\n",
    "        X = [max(vi, epsilon)::Float64 for vi in L]\n",
    "        L = diagm(X)\n",
    "        return (V'*L*V)\n",
    "    end\n",
    "    \n",
    "    # Default mode is RELATIVE tolerance, comment out the line below for ABSOLUTE tolerance\n",
    "    tolerance = tolerance*norm(g(x))\n",
    "\n",
    "    while norm(g(x)) > tolerance && iter < max_iter\n",
    "        if isposdef(H(x))\n",
    "            HH = H(x)\n",
    "        else\n",
    "            HH = fixnonposdefmat(H(x))\n",
    "        end\n",
    "        dx = -HH\\g(x)\n",
    "        t = 1\n",
    "        while f(x+t*dx) > f(x) + alpha*t*sum(g(x).*dx)\n",
    "            t = beta*t\n",
    "        end\n",
    "        x = x + t*dx\n",
    "        iter = iter + 1\n",
    "    end\n",
    "    if iter > max_iter\n",
    "        error(\"Exceeding max iterations\")\n",
    "    end\n",
    "    return x\n",
    "end\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Minimization algorithm using BFGS\n",
    "# f  the objective function\n",
    "# g  the gradient of f\n",
    "# H  the initial guess of the INVERSE HESSIAN, you should set it to the identify matrix if you don't know\n",
    "# x  an initial testing point\n",
    "# tolerance a small number\n",
    "function minimizeBFGS(f,g, H, x::Vector, tolerance=DEFAULT_TOLERANCE, max_iter = DEFAULT_MAX_ITER)\n",
    "    alpha =0.3\n",
    "    beta = 0.7\n",
    "    iter = 0\n",
    "    \n",
    "    I = eye(length(x))\n",
    "    \n",
    "    # Default mode is RELATIVE tolerance, comment out the line below for ABSOLUTE tolerance\n",
    "    # tolerance = tolerance*norm(g(x))\n",
    "\n",
    "    while norm(g(x)) > tolerance && iter < max_iter\n",
    "        dx = -H*g(x)\n",
    "        t = 1\n",
    "        while f(x+t*dx) > f(x) + alpha*t*sum(g(x).*dx)\n",
    "            t = beta*t\n",
    "        end\n",
    "        x1 = x + t*dx\n",
    "        s = t*dx\n",
    "        y = g(x1) - g(x)\n",
    "        rho = 1/dot(s,y)\n",
    "        H = (I- rho* s *(y'))*H*(I-rho*y*(s')) + rho*s*(s')\n",
    "        x = x1\n",
    "        iter = iter + 1\n",
    "    end\n",
    "    if iter > max_iter\n",
    "        error(\"Exceeding max iterations\")\n",
    "    end\n",
    "    return x\n",
    "end\n",
    "\n",
    "\n",
    "# Minimization algorithm of equality constraint problem using Augmented Lagrangian\n",
    "# f cost function\n",
    "# g gradient of f, as a size n column vector, where n is the dimension of the domain\n",
    "# c equality constraint, as a column function from R^n to R^m. c = [c_1;...;c_m] where c_i are real functions\n",
    "# J the Jacobian matrix of c, m columns by n rows matrix, the i-th column should be the column gradient of c_i\n",
    "#       Hence, multiplying by J on the left is a function from R^m to R^n\n",
    "function minimizeAL(f,g, c, J, x, y, stopping_condition = DEFAULT_TOLERANCE)\n",
    "    rho = 1\n",
    "    eta = 0.6\n",
    "    # n by n identity matrix\n",
    "    I = eye(length(x))\n",
    "    \n",
    "    # iteration\n",
    "    k = 0\n",
    "\n",
    "    function converged()\n",
    "        norm(c(x)) < stopping_condition && norm(g(x)- J(x)*y) < stopping_condition\n",
    "    end\n",
    "    while converged() == false\n",
    "        # Augmented Lagrangian (AL) function\n",
    "        L(x) = f(x) - dot(c(x),y) + (rho/2)* norm(c(x))^2\n",
    "        # Derivative of the AL function with respect to x\n",
    "        Lx(x) = g(x) - J(x)*(y- rho *c(x))\n",
    "\n",
    "        x = minimizeBFGS(L, Lx, I, x, eta^k)\n",
    "        if norm(c(x)) < eta^k\n",
    "            y = y - rho*c(x)\n",
    "        else\n",
    "            rho = 10*rho\n",
    "        end\n",
    "        k+=1\n",
    "    end\n",
    "    @printf \"Augmented Lagrangian with quasi-Newton method converged in \\n %d iterations\\n\" k\n",
    "    return (x,y)\n",
    "end\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Augmented Lagrangian with quasi-Newton method converged in \n",
      " 8 iterations\n",
      "x* = [0.9994304481550594,0.9992496873366561]\n",
      "f(x*) = 0.000000 \n"
     ]
    }
   ],
   "source": [
    "# Hock-Schittkowski problem 6\n",
    "f(x) = (1-x[1])^2\n",
    "g(x) = [-2(1-x[1]);0]\n",
    "c(x) = 10(x[2]-x[1]^2)\n",
    "J(x) = [-20*x[1];10]\n",
    "x = [-1.2;1]\n",
    "y = 0\n",
    "(x,y) = minimizeAL(f,g,c,J,x,y)\n",
    "@printf \"x* = \"\n",
    "println(x)\n",
    "@printf \"f(x*) = %f \\n\" f(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Augmented Lagrangian with quasi-Newton method converged in \n",
      " 10 iterations\n",
      "x* = [0.001186499998039978,1.7328983297573237]\n",
      "f(x*) = -1.732897 \n"
     ]
    }
   ],
   "source": [
    "# Hock-Schittkowski problem 7\n",
    "f(x) = log(1+x[1]^2) - x[2]\n",
    "g(x) = [2x[1]/(1+x[1]^2);-1]\n",
    "c(x) = (1+x[1]^2)^2 + x[2]^2 - 4\n",
    "J(x) = [2*(1+x[1]^2)*(2x[1]);2x[2]]\n",
    "x = [2;2]\n",
    "y = 0\n",
    "(x,y) = minimizeAL(f,g,c,J,x,y)\n",
    "\n",
    "@printf \"x* = \"\n",
    "println(x)\n",
    "@printf \"f(x*) = %f \\n\" f(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quasinewton method converged in 9 iterations\n",
      "[4.60253602813184,1.9545088015061312]\n",
      "-1\n"
     ]
    }
   ],
   "source": [
    "# Hock-Schittkowski problem 8\n",
    "f(x) = -1\n",
    "g(x) = [0,0]\n",
    "c(x) = [x[1]^2+x[2]^2 - 25, x[1]*x[2] - 9]\n",
    "J(x) = [2x[1] x[2] ; 2x[2] x[1]]\n",
    "x = [2,1]\n",
    "y = [0,0]\n",
    "(x,y) = minimizeAL(f,g,c,J,x,y)\n",
    "\n",
    "@printf \"x* = \"\n",
    "println(x)\n",
    "@printf \"f(x*) = %f \\n\" f(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quasinewton method converged in 9 iterations\n",
      "[20.915646594121846,27.888684794562934]\n",
      "-0.4996308791289964\n"
     ]
    }
   ],
   "source": [
    "# Hock-Schittkowski problem 9\n",
    "f(x) = sin(pi*x[1]/12)*cos(pi*x[2]/16)\n",
    "g(x) = [(pi/12)*cos(pi*x[1]/12)*cos(pi*x[2]/16),-(pi/16)*sin(pi*x[1]/12)*sin(pi*x[2]/16)]\n",
    "c(x) = 4x[1] - 3x[2]\n",
    "J(x) = [4;-3]\n",
    "x = [2,1]\n",
    "y = 0\n",
    "(x,y) = minimizeAL(f,g,c,J,x,y)\n",
    "\n",
    "@printf \"x* = \"\n",
    "println(x)\n",
    "@printf \"f(x*) = %f \\n\" f(x)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 0.4.0",
   "language": "julia",
   "name": "julia-0.4"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "0.4.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
